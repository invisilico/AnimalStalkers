{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ErrorCorrection_Initialization",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/invisilico/AnimalStalkers/blob/main/ErrorCorrection_Initialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "U6wuHsokMboh"
      },
      "source": [
        "# Animal Stalkers' Project\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Error correction by non-random initialization\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWDPLiCeTXHq",
        "cellView": "form"
      },
      "source": [
        "#@title Quick Set-Up\n",
        "!pip install opencv-python --quiet\n",
        "!pip install google.colab --quiet\n",
        "import matplotlib as mpl\n",
        "import re\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        " \n",
        "#helper functions\n",
        "def PlotLabelAndPrediction(batch, hm_pred, idx=None, title_string=''):\n",
        "  \"\"\"\n",
        "  PlotLabelAndPrediction(batch,pred,idx=None):\n",
        "  Plot the input, labels, and predictions for a batch.\n",
        "  \"\"\"\n",
        "  isbatch = isinstance(batch['id'], torch.Tensor)\n",
        " \n",
        "  if idx is None and isbatch:\n",
        "    idx = range(len(batch['id']))\n",
        "  if isbatch:\n",
        "    n = len(idx)\n",
        "  else:\n",
        "    n = 1\n",
        "    idx = [None,]\n",
        "  locs_pred = heatmap2landmarks(hm_pred.cpu().numpy())\n",
        "  for i in range(n):\n",
        " \n",
        "    plt.subplot(n, 4, 4*i + 1)\n",
        "    im = COCODataset.get_image(batch, idx[i])\n",
        "    plt.imshow(im,cmap='gray')\n",
        "    locs = COCODataset.get_landmarks(batch, idx[i])\n",
        "    for k in range(train_dataset.nlandmarks):\n",
        "      plt.plot(locs[k, 0], locs[k, 1],\n",
        "               marker='.', color=colors[k],\n",
        "               markerfacecolor=colors[k])\n",
        "    if isbatch:\n",
        "      batchid = batch['id'][i]\n",
        "    else:\n",
        "      batchid = batch['id']\n",
        "    plt.title(f\"{title_string}{batchid}\")\n",
        " \n",
        "    plt.subplot(n, 4, 4*i + 2)\n",
        "    plt.imshow(im,cmap='gray')\n",
        "    locs = COCODataset.get_landmarks(batch, idx[i])\n",
        "    if isbatch:\n",
        "      locs_pred_curr = locs_pred[i, ...]\n",
        "    else:\n",
        "      locs_pred_curr = locs_pred\n",
        "    for k in range(train_dataset.nlandmarks):\n",
        "      plt.plot(locs_pred_curr[k, 0], locs_pred_curr[k, 1],\n",
        "               marker='.', color=colors[k],\n",
        "               markerfacecolor=colors[k])\n",
        "    if i == 0: plt.title('pred')\n",
        " \n",
        "    plt.subplot(n, 4, 4*i + 3)\n",
        "    hmim = COCODataset.get_heatmap_image(batch, idx[i])\n",
        "    plt.imshow(hmim)\n",
        "    if i == 0: plt.title('label')\n",
        " \n",
        "    plt.subplot(n, 4, 4*i + 4)\n",
        "    if isbatch:\n",
        "      predcurr = hm_pred[idx[i], ...]\n",
        "    else:\n",
        "      predcurr = hm_pred\n",
        "    plt.imshow(heatmap2image(predcurr.cpu().numpy(), colors=colors))\n",
        "    if i == 0: plt.title('pred')\n",
        " \n",
        "#Setting GPU and Version\n",
        "#print(f\"numpy version: {np.__version__}\")\n",
        "print(\"GPU availability:\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#print(f\"\\ntorch version: {torch.__version__}\")\n",
        " \n",
        "# Download the data\n",
        "import os, requests, tarfile\n",
        " \n",
        "fname = 'fly_bubble_20201204.tar.gz'\n",
        "url = 'https://osf.io/q7vhy/download'\n",
        "datadir = 'view0'\n",
        " \n",
        "if not os.path.exists(fname):\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  with open(fname, 'wb') as ftar:\n",
        "    ftar.write(r.content)\n",
        "  print('\\nFly pose data has been downloaded.')\n",
        " \n",
        "# Untar fly pose data\n",
        "if not os.path.exists(datadir):\n",
        "  with tarfile.open(fname, 'r') as f:\n",
        "    f.extractall('.')  # specify which folder to extract to\n",
        "    # remove tar file\n",
        "    os.remove(fname)\n",
        "    print('Fly pose data has been unzipped.')\n",
        "else:\n",
        "  print('Fly pose data already unzipped.')\n",
        " \n",
        "# Make sure all the data exists\n",
        "traindir = os.path.join(datadir, 'train')\n",
        "trainannfile = os.path.join(datadir, 'train_annotations.json')\n",
        "testdir = os.path.join(datadir, 'test')\n",
        "testannfile = os.path.join(datadir, 'test_annotations.json')\n",
        "assert os.path.exists(traindir) and os.path.exists(testdir) and os.path.exists(trainannfile) and os.path.exists(testannfile), 'Could not find all necessary data after unzipping'\n",
        "print('\\nFound all the fly pose data')\n",
        " \n",
        "# Read annotation information\n",
        "with open(trainannfile) as f:\n",
        "  trainann = json.load(f)\n",
        "f.close()\n",
        "ntrainims = len(trainann['images'])\n",
        "# Make sure we have all the images\n",
        "t = glob(os.path.join(traindir,'*.png'))\n",
        "print(f\"N. train images = {ntrainims}, number of images unzipped = {len(t)}\")\n",
        "assert ntrainims == len(t), 'number of annotations and number of images do not match'\n",
        " \n",
        "# get some features of the data set\n",
        "i = 0\n",
        "filestr = trainann['images'][0]['file_name']\n",
        "imfile = os.path.join(traindir,filestr)\n",
        "im = cv2.imread(imfile, cv2.IMREAD_UNCHANGED)\n",
        "imsize = im.shape\n",
        "if len(imsize) == 2:\n",
        "  imsize += (1, )\n",
        "print(f\"input image size: {imsize}\")\n",
        " \n",
        "landmark_names = ['head_fc', 'head_bl', 'head_br', 'thorax_fr', 'thorax_fl',\n",
        "                  'thorax_bc', 'abdomen', 'leg_ml_in', 'leg_ml_c',' leg_mr_in',\n",
        "                  'leg_mr_c', 'leg_fl_tip', 'leg_ml_tip', 'leg_bl_tip',\n",
        "                  'leg_br_tip','leg_mr_tip','leg_fr_tip']\n",
        "nlandmarks = trainann['annotations'][0]['num_keypoints']\n",
        "assert nlandmarks == len(landmark_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Wtc9vD46Mbon"
      },
      "source": [
        "---\n",
        "# Determine a working ROI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IeqVk_8_sVr"
      },
      "source": [
        "I first tried sequentially averaging through multiple images and realized it did not give me a good idea of the nearby flies, so I switched to sequentially blending images together, in something like the darken mode. Then I actually implemented Darken (choosing pixels that are darker from each subsequent frame)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxssymNfYFQh",
        "cellView": "form"
      },
      "source": [
        "#@title overlay some images to fit ellipse or circle\n",
        " \n",
        "nimsshow =  10#@param# {type:\"integer\"} # number of images to plot\n",
        "imsshow = np.random.choice(ntrainims,nimsshow)\n",
        "im = np.zeros(shape=(181,181),dtype = int)\n",
        " \n",
        "for i in range(nimsshow):\n",
        "  filestr = trainann['images'][imsshow[i]]['file_name']\n",
        "  imfile = os.path.join(traindir, filestr)\n",
        "  imdata = 1 - cv2.imread(imfile, cv2.IMREAD_UNCHANGED)\n",
        "  im = im + 0.1*imdata\n",
        " \n",
        "fig = plt.figure(figsize=(4,4), dpi=100) \n",
        "plt.imshow(im, cmap=\"gray_r\", extent = [-1,1,-1,1])\n",
        " \n",
        "##@markdown for circle fit:\n",
        "circ_r = 0.55 #@param#\n",
        "circle = plt.Circle((0, 0), circ_r,fc='None', edgecolor='r')\n",
        "plt.gca().add_patch(circle)\n",
        "##@markdown for ellipse fit:\n",
        "ellipse_width =  0.8#@param#\n",
        "ellipse_height =  1.2#@param#\n",
        "from matplotlib.patches import Ellipse\n",
        "ellipse = Ellipse(xy=(0,0), width=ellipse_width, height=ellipse_height, edgecolor='g', fc='None')\n",
        "plt.gca().add_patch(ellipse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh9uds5WAM5y",
        "cellView": "form"
      },
      "source": [
        "#@title Actually \"Darken\" images to fit ellipse or circle\n",
        " \n",
        "nimsshow =  4216#@param# {type:\"integer\"} # number of images to plot\n",
        "imsshow = np.random.choice(ntrainims,nimsshow)\n",
        "im = np.zeros(shape=(181,181),dtype = int)\n",
        " \n",
        "for i in range(nimsshow):\n",
        "  filestr = trainann['images'][imsshow[i]]['file_name']\n",
        "  imfile = os.path.join(traindir, filestr)\n",
        "  imdata = 1 - cv2.imread(imfile, cv2.IMREAD_UNCHANGED)\n",
        "  im = np.maximum(im,imdata)\n",
        " \n",
        "fig = plt.figure(figsize=(4,4), dpi=100) \n",
        "plt.imshow(im, cmap=\"gray_r\", extent = [-1,1,-1,1])\n",
        " \n",
        "##@markdown for circle fit:\n",
        "circ_r = 0.55 #@param#\n",
        "circle = plt.Circle((0, 0), circ_r,fc='None', edgecolor='r')\n",
        "plt.gca().add_patch(circle)\n",
        "##@markdown for ellipse fit:\n",
        "ellipse_width =  0.8#@param#\n",
        "ellipse_height = 1.2 #@param#\n",
        "from matplotlib.patches import Ellipse\n",
        "ellipse = Ellipse(xy=(0,0), width=ellipse_width, height=ellipse_height, edgecolor='g', fc='None')\n",
        "plt.gca().add_patch(ellipse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMiZMof-ddbZ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Data loading into dataset class\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "wnsCFHALMboo",
        "cellView": "form"
      },
      "source": [
        "#@title define a dataset class to load the data\n",
        " \n",
        "def heatmap2image(hm, cmap='jet', colors=None):\n",
        "  \"\"\"\n",
        "  heatmap2image(hm,cmap='jet',colors=None)\n",
        "  Creates and returns an image visualization from landmark heatmaps. Each\n",
        "  landmark is colored according to the input cmap/colors.\n",
        "  Inputs:\n",
        "    hm: nlandmarks x height x width ndarray, dtype=float in the range 0 to 1.\n",
        "    hm[p,i,j] is a score indicating how likely it is that the pth landmark\n",
        "    is at pixel location (i,j).\n",
        "    cmap: string.\n",
        "    Name of colormap for defining colors of landmark points. Used only if colors\n",
        "    is None.\n",
        "    Default: 'jet'\n",
        "    colors: list of length nlandmarks.\n",
        "    colors[p] is an ndarray of size (4,) indicating the color to use for the\n",
        "    pth landmark. colors is the output of matplotlib's colormap functions.\n",
        "    Default: None\n",
        "  Output:\n",
        "    im: height x width x 3 ndarray\n",
        "    Image representation of the input heatmap landmarks.\n",
        "  \"\"\"\n",
        "  hm = np.maximum(0., np.minimum(1. ,hm))\n",
        "  im = np.zeros((hm.shape[1], hm.shape[2], 3))\n",
        "  if colors is None:\n",
        "    if isinstance(cmap, str):\n",
        "      cmap = matplotlib.cm.get_cmap(cmap)\n",
        "    colornorm = matplotlib.colors.Normalize(vmin=0, vmax=hm.shape[0])\n",
        "    colors = cmap(colornorm(np.arange(hm.shape[0])))\n",
        "  for i in range(hm.shape[0]):\n",
        "    color = colors[i]\n",
        "    for c in range(3):\n",
        "      im[..., c] = im[..., c] + (color[c] * .7 + .3) * hm[i, ...]\n",
        "  im = np.minimum(1.,im)\n",
        " \n",
        "  return im\n",
        " \n",
        " \n",
        "class COCODataset(torch.utils.data.Dataset):\n",
        "  \"\"\"\n",
        "  COCODataset\n",
        "  Torch Dataset based on the COCO keypoint file format.\n",
        "  \"\"\"\n",
        " \n",
        "  def __init__(self, annfile, datadir=None, label_sigma=3.,\n",
        "               transform=None, landmarks=None):\n",
        "    \"\"\"\n",
        "    Constructor\n",
        "    This must be defined in every Torch Dataset and can take any inputs you\n",
        "    want it to.\n",
        "    Inputs:\n",
        "      annfile: string\n",
        "      Path to json file containing annotations.\n",
        "      datadir: string\n",
        "      Path to directory containing images. If None, images are assumed to be in\n",
        "      the working directory.\n",
        "      Default: None\n",
        "      label_sigma: scalar float\n",
        "      Standard deviation in pixels of Gaussian to be used to make the landmark\n",
        "      heatmap.\n",
        "      Default: 3.\n",
        "      transform: None\n",
        "      Not used currently\n",
        "      landmarks: ndarray (or list, something used for indexing into ndarray)\n",
        "      Indices of landmarks available to use in this dataset. Reducing the\n",
        "      landmarks used can make training faster and require less memory, and is\n",
        "      useful for testing code. If None, all landmarks are used.\n",
        "      Default: None\n",
        "    \"\"\"\n",
        " \n",
        "    # read in the annotations from the json file\n",
        "    with open(annfile) as f:\n",
        "      self.ann = json.load(f)\n",
        "    # where the images are\n",
        "    self.datadir = datadir\n",
        " \n",
        "    # landmarks to use\n",
        "    self.nlandmarks_all = self.ann['annotations'][0]['num_keypoints']\n",
        "    if landmarks is None:\n",
        "      self.nlandmarks = self.nlandmarks_all\n",
        "    else:\n",
        "      self.nlandmarks = len(landmarks)\n",
        "    self.landmarks = landmarks\n",
        " \n",
        "    # for data augmentation/rescaling\n",
        "    self.transform = transform\n",
        " \n",
        "    # output will be heatmap images, one per landmark, with Gaussian values\n",
        "    # around the landmark location -- precompute some stuff for that\n",
        "    self.label_filter = None\n",
        "    self.label_filter_r = 1\n",
        "    self.label_filter_d = 3\n",
        "    self.label_sigma = label_sigma\n",
        "    self.init_label_filter()\n",
        " \n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Overloaded len function.\n",
        "    This must be defined in every Torch Dataset and must take only self\n",
        "    as input.\n",
        "    Returns the number of examples in the dataset.\n",
        "    \"\"\"\n",
        "    return len(self.ann['images'])\n",
        " \n",
        "  def __getitem__(self, item):\n",
        "    \"\"\"\n",
        "    Overloaded getitem function.\n",
        "    This must be defined in every Torch Dataset and must take only self\n",
        "    and item as input. It returns example number item.\n",
        "    item: scalar integer.\n",
        "    The output example is a dict with the following fields:\n",
        "    image: torch float32 tensor of size ncolors x height x width\n",
        "    landmarks: nlandmarks x 2 float ndarray\n",
        "    heatmaps: torch float32 tensor of size nlandmarks x height x width\n",
        "    id: scalar integer, contains item\n",
        "    \"\"\"\n",
        " \n",
        "    # read in the image for training example item\n",
        "    # and convert to a torch tensor\n",
        "    filename = self.ann['images'][item]['file_name']\n",
        "    if self.datadir is not None:\n",
        "      filename = os.path.join(self.datadir, filename)\n",
        "    assert os.path.exists(filename)\n",
        "    im = torch.from_numpy(cv2.imread(filename, cv2.IMREAD_UNCHANGED))\n",
        " \n",
        "    # convert to float32 in the range 0. to 1.\n",
        "    if im.dtype == float:\n",
        "      pass\n",
        "    elif im.dtype == torch.uint8:\n",
        "      im = im.float() / 255.\n",
        "    elif im.dtype == torch.uint16:\n",
        "      im = im.float() / 65535.\n",
        "    else:\n",
        "      print('Cannot handle im type '+str(im.dtype))\n",
        "      raise TypeError\n",
        " \n",
        "    imsz = im.shape\n",
        "    # convert to a tensor of size ncolors x h x w\n",
        "    if im.dim() == 3:\n",
        "      im = torch.transpose(im, [2, 0, 1])  # now 3 x h x w\n",
        "    else:\n",
        "      im = torch.unsqueeze(im, 0)  # now 1 x h x w\n",
        " \n",
        "    # landmark locations\n",
        "    locs = np.reshape(self.ann['annotations'][item]['keypoints'],\n",
        "                      [self.nlandmarks_all, 3])\n",
        "    locs = locs[:, :2]\n",
        "    if self.landmarks is not None:\n",
        "      locs = locs[self.landmarks, :]\n",
        " \n",
        "    # create heatmap target prediction\n",
        "    heatmaps = self.make_heatmap_target(locs, imsz)\n",
        " \n",
        "    # return a dict with the following fields:\n",
        "    # image: torch float32 tensor of size ncolors x height x width\n",
        "    # landmarks: nlandmarks x 2 float ndarray\n",
        "    # heatmaps: torch float32 tensor of size nlandmarks x height x width\n",
        "    # id: scalar integer, contains item\n",
        "    features = {'image':im,\n",
        "                'landmarks':locs.astype(np.float32),\n",
        "                'heatmaps':heatmaps,\n",
        "                'id':item}\n",
        " \n",
        "    return features\n",
        " \n",
        "  def init_label_filter(self):\n",
        "    \"\"\"\n",
        "    init_label_filter(self)\n",
        "    Helper function\n",
        "    Create a Gaussian filter for the heatmap target output\n",
        "    \"\"\"\n",
        "    # radius of the filter\n",
        "    self.label_filter_r = max(int(round(3 * self.label_sigma)), 1)\n",
        "    # diameter of the filter\n",
        "    self.label_filter_d = 2 * self.label_filter_r + 1\n",
        " \n",
        "    # allocate\n",
        "    self.label_filter = np.zeros([self.label_filter_d, self.label_filter_d])\n",
        "    # set the middle pixel to 1.\n",
        "    self.label_filter[self.label_filter_r, self.label_filter_r] = 1.\n",
        "    # blur with a Gaussian\n",
        "    self.label_filter = cv2.GaussianBlur(self.label_filter,\n",
        "                                         (self.label_filter_d,\n",
        "                                          self.label_filter_d),\n",
        "                                         self.label_sigma)\n",
        "    # normalize\n",
        "    self.label_filter = self.label_filter / np.max(self.label_filter)\n",
        "    # convert to torch tensor\n",
        "    self.label_filter = torch.from_numpy(self.label_filter)\n",
        " \n",
        "  def make_heatmap_target(self, locs, imsz):\n",
        "    \"\"\"\n",
        "    make_heatmap_target(self,locs,imsz):\n",
        "    Helper function\n",
        "    Creates the heatmap tensor of size imsz corresponding to landmark locations locs\n",
        "    Inputs:\n",
        "      locs: nlandmarks x 2 ndarray\n",
        "      Locations of landmarks\n",
        "      imsz: image shape\n",
        "    Returns:\n",
        "      target: torch tensor of size nlandmarks x imsz[0] x imsz[1]\n",
        "      Heatmaps corresponding to locs\n",
        "    \"\"\"\n",
        "    # allocate the tensor\n",
        "    target = torch.zeros((locs.shape[0], imsz[0], imsz[1]), dtype=torch.float32)\n",
        "    # loop through landmarks\n",
        "    for i in range(locs.shape[0]):\n",
        "      # location of this landmark to the nearest pixel\n",
        "      x = int(np.round(locs[i, 0])) # losing sub-pixel accuracy\n",
        "      y = int(np.round(locs[i, 1]))\n",
        "      # edges of the Gaussian filter to place, minding border of image\n",
        "      x0 = np.maximum(0, x - self.label_filter_r)\n",
        "      x1 = np.minimum(imsz[1] - 1, x + self.label_filter_r)\n",
        "      y0 = np.maximum(0, y - self.label_filter_r)\n",
        "      y1 = np.minimum(imsz[0] - 1, y + self.label_filter_r)\n",
        "      # crop filter if it goes outside of the image\n",
        "      fil_x0 = self.label_filter_r - (x - x0)\n",
        "      fil_x1 = self.label_filter_d - (self.label_filter_r - (x1 - x))\n",
        "      fil_y0 = self.label_filter_r - (y - y0)\n",
        "      fil_y1 = self.label_filter_d - (self.label_filter_r - (y1 - y))\n",
        "      # copy the filter to the relevant part of the heatmap image\n",
        "      target[i, y0:y1 + 1, x0:x1 + 1] = self.label_filter[fil_y0:fil_y1 + 1,\n",
        "                                                          fil_x0:fil_x1 + 1]\n",
        " \n",
        "    return target\n",
        " \n",
        "  @staticmethod\n",
        "  def get_image(d, i=None):\n",
        "    \"\"\"\n",
        "    static function, used for visualization\n",
        "    COCODataset.get_image(d,i=None)\n",
        "    Returns an image usable with plt.imshow()\n",
        "    Inputs:\n",
        "      d: if i is None, item from a COCODataset.\n",
        "      if i is a scalar, batch of examples from a COCO Dataset returned\n",
        "      by a DataLoader.\n",
        "      i: Index of example into the batch d, or None if d is a single example\n",
        "    Returns the ith image from the patch as an ndarray plottable with\n",
        "    plt.imshow()\n",
        "    \"\"\"\n",
        "    if i is None:\n",
        "      im = np.squeeze(np.transpose(d['image'].numpy(), (1, 2, 0)), axis=2)\n",
        "    else:\n",
        "      im = np.squeeze(np.transpose(d['image'][i,...].numpy(), (1, 2, 0)), axis=2)\n",
        "    return im\n",
        " \n",
        "  @staticmethod\n",
        "  def get_landmarks(d, i=None):\n",
        "    \"\"\"\n",
        "    static helper function\n",
        "    COCODataset.get_landmarks(d,i=None)\n",
        "    Returns a nlandmarks x 2 ndarray indicating landmark locations.\n",
        "    Inputs:\n",
        "      d: if i is None, item from a COCODataset.\n",
        "      if i is a scalar, batch of examples from a COCO Dataset returned\n",
        "      by a DataLoader.\n",
        "      i: Index of example into the batch d, or None if d is a single example\n",
        "    \"\"\"\n",
        "    if i is None:\n",
        "      locs = d['landmarks']\n",
        "    else:\n",
        "      locs = d['landmarks'][i]\n",
        "    return locs\n",
        " \n",
        "  @staticmethod\n",
        "  def get_heatmap_image(d, i, cmap='jet', colors=None):\n",
        "    \"\"\"\n",
        "    static function, used for visualization\n",
        "    COCODataset.get_heatmap_image(d,i=None)\n",
        "    Returns an image visualization of heatmaps usable with plt.imshow()\n",
        "    Inputs:\n",
        "      d: if i is None, item from a COCODataset.\n",
        "      if i is a scalar, batch of examples from a COCO Dataset returned\n",
        "      by a DataLoader.\n",
        "      i: Index of example into the batch d, or None if d is a single example\n",
        "      Returns the ith heatmap from the patch as an ndarray plottable with\n",
        "      plt.imshow()\n",
        "      cmap: string.\n",
        "      Name of colormap for defining colors of landmark points. Used only if colors\n",
        "      is None.\n",
        "      Default: 'jet'\n",
        "      colors: list of length nlandmarks.\n",
        "      colors[p] is an ndarray of size (4,) indicating the color to use for the\n",
        "      pth landmark. colors is the output of matplotlib's colormap functions.\n",
        "      Default: None\n",
        "    Output:\n",
        "      im: height x width x 3 ndarray\n",
        "      Image representation of the input heatmap landmarks.\n",
        "    \"\"\"\n",
        "    if i is None:\n",
        "      hm = d['heatmaps']\n",
        "    else:\n",
        "      hm = d['heatmaps'][i, ...]\n",
        "    hm = hm.numpy()\n",
        "    im = heatmap2image(hm, cmap=cmap, colors=colors)\n",
        " \n",
        "    return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "wgyMc0CtMbop",
        "cellView": "form"
      },
      "source": [
        "#@title instantiate train data loader\n",
        "\n",
        "# only use a subset of the landmarks\n",
        "#landmarks = np.where(list(map(lambda x: x in ['head_fc','leg_fl_tip','leg_fr_tip'],landmark_names)))[0]\n",
        "# use all the landmarks\n",
        "landmarks = None\n",
        "\n",
        "train_dataset = COCODataset(trainannfile, datadir=traindir, landmarks=landmarks)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=2,\n",
        "                                               shuffle=True)\n",
        "nimsshow = 6\n",
        "# plot example images using the dataloader\n",
        "fig = plt.figure(figsize=(nimsshow * 4, 8), dpi=100)\n",
        "\n",
        "# choose some colors for each landmark\n",
        "cmap = matplotlib.cm.get_cmap('jet')\n",
        "colornorm = matplotlib.colors.Normalize(vmin=0, vmax=train_dataset.nlandmarks)\n",
        "colors = cmap(colornorm(np.arange(train_dataset.nlandmarks)))\n",
        "\n",
        "count = 0\n",
        "for i_batch, sample_batch in enumerate(train_dataloader):\n",
        "  for j in range(len(sample_batch['id'])):\n",
        "    plt.subplot(2, nimsshow, count + 1)\n",
        "    # use our helper functions for getting and formatting data from the batch\n",
        "    im = COCODataset.get_image(sample_batch, j)\n",
        "    locs = COCODataset.get_landmarks(sample_batch, j)\n",
        "    plt.imshow(im, cmap='gray')\n",
        "    for k in range(train_dataset.nlandmarks):\n",
        "      plt.plot(locs[k, 0], locs[k, 1], marker='.', color=colors[k],\n",
        "               markerfacecolor=colors[k])\n",
        "    plt.title('%d'%sample_batch['id'][j])\n",
        "    hmim = COCODataset.get_heatmap_image(sample_batch, j, colors=colors)\n",
        "    plt.subplot(2, nimsshow, count + 1 + nimsshow)\n",
        "    plt.imshow(hmim)\n",
        "    count += 1\n",
        "    if count >= nimsshow:\n",
        "      break\n",
        "  if count >= nimsshow:\n",
        "    break\n",
        "\n",
        "# Show the structure of a batch\n",
        "# print(sample_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "GSxDQuOxMbop"
      },
      "source": [
        "---\n",
        "# Architectures\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "CWRZdvJiMbop",
        "cellView": "form"
      },
      "source": [
        "#@title Define network structure - UNet\n",
        "# Copy-paste & modify from https://github.com/milesial/Pytorch-UNet\n",
        "\n",
        "# The UNet is defined modularly.\n",
        "# It is a series of downsampling layers defined by the module Down\n",
        "# followed by upsampling layers defined by the module Up. The output is\n",
        "# a convolutional layer with an output channel for each landmark, defined by\n",
        "# the module OutConv.\n",
        "# Each down and up layer is actually two convolutional layers with\n",
        "# a ReLU nonlinearity and batch normalization, defined by the module\n",
        "# DoubleConv.\n",
        "# The Down module consists of a 2x2 max pool layer followed by the DoubleConv\n",
        "# module.\n",
        "# The Up module consists of an upsampling, either defined via bilinear\n",
        "# interpolation (bilinear=True), or a learned convolutional transpose, followed\n",
        "# by a DoubleConv module.\n",
        "# The Output layer is a single 2-D convolutional layer with no nonlinearity.\n",
        "# The nonlinearity is incorporated into the network loss function.\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "  \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "    super().__init__()\n",
        "    if not mid_channels:\n",
        "        mid_channels = out_channels\n",
        "    self.double_conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(mid_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "  \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.maxpool_conv = nn.Sequential(\n",
        "        nn.MaxPool2d(2),\n",
        "        DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "  \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "    super().__init__()\n",
        "\n",
        "    # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "    if bilinear:\n",
        "      self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "      self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "    else:\n",
        "      self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "      self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "  def forward(self, x1, x2):\n",
        "    x1 = self.up(x1)\n",
        "    # input is CHW\n",
        "    diffY = x2.size()[2] - x1.size()[2]\n",
        "    diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "    x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                    diffY // 2, diffY - diffY // 2])\n",
        "    # if you have padding issues, see\n",
        "    # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "    # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "    x = torch.cat([x2, x1], dim=1)\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(OutConv, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "# copy-pasted and modified from unet_model.py\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, n_channels, n_landmarks, bilinear=True):\n",
        "    super(UNet, self).__init__()\n",
        "    self.n_channels = n_channels\n",
        "    self.n_landmarks = n_landmarks\n",
        "    self.bilinear = bilinear\n",
        "    self.nchannels_inc = 8\n",
        "\n",
        "    # define the layers\n",
        "\n",
        "    # number of channels in the first layer\n",
        "    nchannels_inc = self.nchannels_inc\n",
        "    # increase the number of channels by a factor of 2 each layer\n",
        "    nchannels_down1 = nchannels_inc*2\n",
        "    nchannels_down2 = nchannels_down1*2\n",
        "    nchannels_down3 = nchannels_down2*2\n",
        "    # decrease the number of channels by a factor of 2 each layer\n",
        "    nchannels_up1 = nchannels_down3//2\n",
        "    nchannels_up2 = nchannels_up1//2\n",
        "    nchannels_up3 = nchannels_up2//2\n",
        "\n",
        "    if bilinear:\n",
        "      factor = 2\n",
        "    else:\n",
        "      factor = 1\n",
        "\n",
        "    self.layer_inc = DoubleConv(n_channels, nchannels_inc)\n",
        "\n",
        "    self.layer_down1 = Down(nchannels_inc, nchannels_down1)\n",
        "    self.layer_down2 = Down(nchannels_down1, nchannels_down2)\n",
        "    self.layer_down3 = Down(nchannels_down2, nchannels_down3//factor)\n",
        "\n",
        "    self.layer_up1 = Up(nchannels_down3, nchannels_up1//factor, bilinear)\n",
        "    self.layer_up2 = Up(nchannels_up1, nchannels_up2//factor, bilinear)\n",
        "    self.layer_up3 = Up(nchannels_up2, nchannels_up3//factor, bilinear)\n",
        "\n",
        "    self.layer_outc = OutConv(nchannels_up3//factor, self.n_landmarks)\n",
        "\n",
        "  def forward(self, x, verbose=False):\n",
        "    x1 = self.layer_inc(x)\n",
        "    if verbose: print(f'inc: shape = {x1.shape}')\n",
        "    x2 = self.layer_down1(x1)\n",
        "    if verbose:print(f'inc: shape = {x2.shape}')\n",
        "    x3 = self.layer_down2(x2)\n",
        "    if verbose: print(f'inc: shape = {x3.shape}')\n",
        "    x4 = self.layer_down3(x3)\n",
        "    if verbose: print(f'inc: shape = {x4.shape}')\n",
        "    x = self.layer_up1(x4, x3)\n",
        "    if verbose: print(f'inc: shape = {x.shape}')\n",
        "    x = self.layer_up2(x, x2)\n",
        "    if verbose: print(f'inc: shape = {x.shape}')\n",
        "    x = self.layer_up3(x, x1)\n",
        "    if verbose: print(f'inc: shape = {x.shape}')\n",
        "    logits = self.layer_outc(x)\n",
        "    if verbose: print(f'outc: shape = {logits.shape}')\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def output(self, x, verbose=False):\n",
        "    return torch.sigmoid(self.forward(x, verbose=verbose))\n",
        "\n",
        "  def __str__(self):\n",
        "    s = ''\n",
        "    s += 'inc: '+str(self.layer_inc)+'\\n'\n",
        "    s += 'down1: '+str(self.layer_down1)+'\\n'\n",
        "    s += 'down2: '+str(self.layer_down2)+'\\n'\n",
        "    s += 'down3: '+str(self.layer_down3)+'\\n'\n",
        "    s += 'up1: '+str(self.layer_up1)+'\\n'\n",
        "    s += 'up2: '+str(self.layer_up2)+'\\n'\n",
        "    s += 'up3: '+str(self.layer_up3)+'\\n'\n",
        "    s += 'outc: '+str(self.layer_outc)+'\\n'\n",
        "    return s\n",
        "\n",
        "  def __repr__(self):\n",
        "    return str(self)\n",
        "\n",
        "\n",
        "def heatmap2landmarks(hms):\n",
        "  idx = np.argmax(hms.reshape(hms.shape[:-2] + (hms.shape[-2]*hms.shape[-1], )),\n",
        "                  axis=-1)\n",
        "  locs = np.zeros(hms.shape[:-2] + (2, ))\n",
        "  locs[...,1],locs[...,0] = np.unravel_index(idx,hms.shape[-2:])\n",
        "  return locs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "g6zwGKj1Mboq",
        "cellView": "form"
      },
      "source": [
        "#@title Instantiate the network\n",
        "net = UNet(n_channels=imsize[-1], n_landmarks=train_dataset.nlandmarks)\n",
        "net.to(device=device) # have to be careful about what is done on the CPU vs GPU\n",
        "\n",
        "# try the network out before training\n",
        "batch = next(iter(train_dataloader))\n",
        "with torch.no_grad():\n",
        "  #hms0 = net.output(batch['image'].to(device=device), verbose=True) #verbose\n",
        "  hms0 = net.output(batch['image'].to(device=device)) # made not verbose\n",
        "\n",
        "#fig = plt.figure(figsize=(12, 4*len(batch['id'])), dpi= 100)\n",
        "#PlotLabelAndPrediction(batch, hms0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "oevapjMvMboq",
        "cellView": "form"
      },
      "source": [
        "#@title Train the network\n",
        "\n",
        "# load a network if one is already saved and you want to restart training\n",
        "# savefile = '/content/drive/My Drive/PoseEstimationNets/UNet20210510T140305/Final_epoch4.pth'\n",
        "#@markdown add dir like '/content/drive/My Drive/PoseEstimationNets/UNet20210510T140305/Final_epoch4.pth' if loading\n",
        "savefile = None #@param\n",
        "loadepoch = 0 \n",
        "# savefile = None\n",
        "if savefile is not None:\n",
        "  net.load_state_dict(\n",
        "      torch.load(savefile, map_location=device)\n",
        "      )\n",
        "  m = re.search('[^\\d](?P<epoch>\\d+)\\.pth$', savefile)\n",
        "  if m is None:\n",
        "    print('Could not parse epoch from file name')\n",
        "  else:\n",
        "    loadepoch = int(m['epoch'])\n",
        "    print(f\"Parsed epoch from loaded net file name: {loadepoch}\")\n",
        "  net.to(device=device)\n",
        "savefile = None\n",
        "loadepoch = 0\n",
        "\n",
        "# train the network\n",
        "# following https://github.com/milesial/Pytorch-UNet/blob/master/train.py\n",
        "\n",
        "# parameters related to training the network\n",
        "batchsize = 2 # number of images per batch -- amount of required memory\n",
        "              # for training will increase linearly in batchsize\n",
        "nepochs = 3   # number of times to cycle through all the data during training\n",
        "learning_rate = 0.001 # initial learning rate\n",
        "weight_decay = 1e-8 # how learning rate decays over time\n",
        "momentum = 0.9 # how much to use previous gradient direction\n",
        "nepochs_per_save = 1 # how often to save the network\n",
        "val_frac = 0.1 # what fraction of data to use for validation\n",
        "\n",
        "# where to save the network\n",
        "# make sure to clean these out every now and then, as you will run out of space\n",
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "timestamp = now.strftime('%Y%m%dT%H%M%S')\n",
        "\n",
        "savedir = '/content/PoseEstimationNets'\n",
        "\n",
        "# if the folder does not exist, create it.\n",
        "if not os.path.exists(savedir):\n",
        "  os.mkdir(savedir)\n",
        "\n",
        "checkpointdir = os.path.join(savedir, 'UNet' + timestamp)\n",
        "os.mkdir(checkpointdir)\n",
        "\n",
        "# split into train and validation datasets\n",
        "n_val = int(len(train_dataset) * val_frac)\n",
        "n_train = len(train_dataset) - n_val\n",
        "train, val = torch.utils.data.random_split(train_dataset, [n_train, n_val])\n",
        "train_dataloader = torch.utils.data.DataLoader(train,\n",
        "                                               batch_size=batchsize,\n",
        "                                               shuffle=True)\n",
        "val_dataloader = torch.utils.data.DataLoader(val,\n",
        "                                             batch_size=batchsize,\n",
        "                                             shuffle=False)\n",
        "\n",
        "# gradient descent flavor\n",
        "optimizer = optim.RMSprop(net.parameters(),\n",
        "                          lr=learning_rate,\n",
        "                          weight_decay=weight_decay,\n",
        "                          momentum=momentum)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n",
        "\n",
        "# Following https://github.com/milesial/Pytorch-UNet\n",
        "# Use binary cross entropy loss combined with sigmoid output activation function.\n",
        "# We combine here for numerical improvements\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# store loss per epoch\n",
        "epoch_losses = np.zeros(nepochs)\n",
        "epoch_losses[:] = np.nan\n",
        "\n",
        "# when we last saved the network\n",
        "saveepoch = None\n",
        "\n",
        "# how many gradient descent updates we have made\n",
        "iters = loadepoch*len(train_dataloader)\n",
        "\n",
        "# loop through entire training data set nepochs times\n",
        "for epoch in range(loadepoch, nepochs):\n",
        "  net.train() # put in train mode (affects batchnorm)\n",
        "  epoch_loss = 0\n",
        "  with tqdm(total=ntrainims,\n",
        "            desc=f\"Epoch {epoch + 1}/{nepochs}\",\n",
        "            unit='img') as pbar:\n",
        "\n",
        "    # loop through each batch in the training data\n",
        "    for batch in train_dataloader:\n",
        "      # compute the loss\n",
        "      imgs = batch['image']\n",
        "      imgs = imgs.to(device=device, dtype=torch.float32) # transfer to GPU\n",
        "      hm_labels = batch['heatmaps']\n",
        "      hm_labels = hm_labels.to(device=device, dtype=torch.float32) # transfer to GPU\n",
        "      hm_preds = net(imgs) # evaluate network on batch\n",
        "      loss = criterion(hm_preds,hm_labels) # compute loss\n",
        "      epoch_loss += loss.item()\n",
        "      pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
        "      # gradient descent\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
        "      optimizer.step()\n",
        "      iters += 1\n",
        "\n",
        "      pbar.update(imgs.shape[0])\n",
        "  print(f\"loss (epoch) = {epoch_loss}\")\n",
        "  epoch_losses[epoch] = epoch_loss\n",
        "\n",
        "  # save checkpoint networks every now and then\n",
        "  if epoch % nepochs_per_save == 0:\n",
        "    print(f\"Saving network state at epoch {epoch + 1}\")\n",
        "    # only keep around the last two epochs for space purposes\n",
        "    if saveepoch is not None:\n",
        "      savefile0 = os.path.join(checkpointdir,\n",
        "                               f\"CP_latest_epoch{saveepoch+1}.pth\")\n",
        "      savefile1 = os.path.join(checkpointdir,\n",
        "                               f\"CP_prev_epoch{saveepoch+1}.pth\")\n",
        "      if os.path.exists(savefile0):\n",
        "        try:\n",
        "          os.rename(savefile0,savefile1)\n",
        "        except:\n",
        "          print(f\"Failed to rename checkpoint file {savefile0} to {savefile1}\")\n",
        "    saveepoch = epoch\n",
        "    savefile = os.path.join(checkpointdir,f\"CP_latest_epoch{saveepoch + 1}.pth\")\n",
        "    torch.save(net.state_dict(),\n",
        "               os.path.join(checkpointdir, f\"CP_latest_epoch{epoch + 1}.pth\"))\n",
        "\n",
        "torch.save(net.state_dict(),\n",
        "           os.path.join(checkpointdir, f\"Final_epoch{epoch + 1}.pth\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "OANnhDbcMboq",
        "cellView": "form"
      },
      "source": [
        "#@title try on val images (from training set)\n",
        "net.eval()\n",
        "batch = next(iter(train_dataloader))\n",
        "with torch.no_grad():\n",
        "  train_hms1 = torch.sigmoid(net(batch['image'].to(device)))\n",
        "\n",
        "fig=plt.figure(figsize=(12, 4*train_hms1.shape[0]), dpi= 100)\n",
        "PlotLabelAndPrediction(batch,train_hms1,title_string='Train ')\n",
        "\n",
        "batch = next(iter(val_dataloader))\n",
        "with torch.no_grad():\n",
        "  val_hms1 = net.output(batch['image'].to(device))\n",
        "fig = plt.figure(figsize=(12, 4 * val_hms1.shape[0]), dpi=100)\n",
        "PlotLabelAndPrediction(batch, val_hms1, title_string='Val ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "KxW7dQ4hMboq"
      },
      "source": [
        "---\n",
        "# Evaluation\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmzZgW908MZ8"
      },
      "source": [
        "Identified images wherein the multiple-fly issue occured\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  [Ranked worst to best]\n",
        "  \n",
        "# evaluation set: 3133, 3575, 3766, 3768, 3721, 3764, 3014 \n",
        "\n",
        "# test set      : 1717, 1549, 69, 1497, 910, 1345, 1406, 1191, 681, 647, 388, \n",
        "                  615, 207, 518, 114, 309, 735, 271, 1375, 1470, 384, 1471, \n",
        "                  183, 1205, 926, 502, 1428, 471, 475 \n",
        "\n",
        "# special case from test set > 1784 [Plate edge detected as body part]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "-awQQpbrMboq",
        "cellView": "form"
      },
      "source": [
        "#@title Evaluate the training and validation error\n",
        "\n",
        "def eval_net(net, loader):\n",
        "  net.eval()\n",
        "  n_val = len(loader) * loader.batch_size\n",
        "  errs = None\n",
        "  count = 0\n",
        "\n",
        "  for batch in loader:\n",
        "\n",
        "    with torch.no_grad():\n",
        "      hm_preds = torch.sigmoid(net(batch['image'].to(device))).cpu().numpy()\n",
        "\n",
        "    idx = np.argmax(hm_preds.reshape((hm_preds.shape[0],\n",
        "                                      hm_preds.shape[1],\n",
        "                                      hm_preds.shape[2] * hm_preds.shape[3])),\n",
        "                    axis=2)\n",
        "    loc_preds = np.zeros((hm_preds.shape[0], hm_preds.shape[1], 2))\n",
        "    loc_preds[:, :, 1], loc_preds[:, :, 0] = np.unravel_index(idx,\n",
        "                                                              hm_preds.shape[2:])\n",
        "\n",
        "    loc_labels = batch['landmarks'].numpy()\n",
        "    l2err = np.sqrt(np.sum((loc_preds - loc_labels)**2., axis=2))\n",
        "    idscurr = batch['id'].numpy()\n",
        "\n",
        "    if errs is None:\n",
        "      errs = np.zeros((n_val, l2err.shape[1]))\n",
        "      errs[:] = np.nan\n",
        "      ids = np.zeros(n_val, dtype=int)\n",
        "\n",
        "    errs[count:(count + l2err.shape[0]), :] = l2err\n",
        "    ids[count:(count + l2err.shape[0])] = idscurr\n",
        "    count += l2err.shape[0]\n",
        "\n",
        "  errs = errs[:count, :]\n",
        "  ids = ids[:count]\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  return errs, ids\n",
        "\n",
        "\n",
        "l2err_per_landmark_val, val_ids = eval_net(net, val_dataloader)\n",
        "l2err_per_landmark_train, train_ids = eval_net(net, train_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "uInqLntwMbor",
        "cellView": "form"
      },
      "source": [
        "#@title Plot the error distribution\n",
        "nbins = 25\n",
        "bin_edges = np.linspace(0, np.percentile(l2err_per_landmark_val, 99.),\n",
        "                        nbins + 1)\n",
        "bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2.\n",
        "bin_edges[-1] = np.inf\n",
        "frac_val = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
        "frac_train = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
        "for i in range(l2err_per_landmark_val.shape[1]):\n",
        "  frac_val[:, i], _ = np.histogram(l2err_per_landmark_val[:, i],\n",
        "                                   bin_edges, density=True)\n",
        "  frac_train[:, i], _ = np.histogram(l2err_per_landmark_train[:, i],\n",
        "                                     bin_edges, density=True)\n",
        "\n",
        "fig = plt.figure(figsize=(8, 4 * train_dataset.nlandmarks), dpi=100)\n",
        "for i in range(train_dataset.nlandmarks):\n",
        "  if landmarks is None:\n",
        "    landmark_name = landmark_names[i]\n",
        "  else:\n",
        "    landmark_name = landmark_names[landmarks[i]]\n",
        "  plt.subplot(train_dataset.nlandmarks, 1, i + 1)\n",
        "  hval = plt.plot(bin_centers,\n",
        "                  frac_val[:, i], '.-',\n",
        "                  label='Val', color=colors[i, :])\n",
        "  plt.plot(bin_centers, frac_train[:, i], ':',\n",
        "           label='Train', color=colors[i, :])\n",
        "  plt.legend()\n",
        "  plt.title(f\"{landmark_name} error (px)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "I6auQ7a1Mbor",
        "cellView": "form"
      },
      "source": [
        "#@title Plot examples with big errors\n",
        "idx = np.argsort(-np.sum(l2err_per_landmark_val, axis=1))\n",
        "\n",
        "#for i in range(5):\n",
        "i = 0\n",
        "while -np.sum(l2err_per_landmark_val[idx[i]]) < -100:\n",
        "  d = train_dataset[val_ids[idx[i]]]\n",
        "  img = d['image'].unsqueeze(0)\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    pred = net.output(img.to(device))\n",
        "\n",
        "  fig=plt.figure(figsize=(12, 4)) #, dpi=100)\n",
        "  with np.printoptions(precision=2):\n",
        "    errstr = str(-np.sum(l2err_per_landmark_val[idx[i]]))\n",
        "  PlotLabelAndPrediction(d,pred[0, ...],title_string='Err = %s '%errstr)  #,title_string='Err = %s '%errstr)\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "oBYlh5_nMbor",
        "cellView": "form"
      },
      "source": [
        "#@title  Visualize the first layer of convolutional features\n",
        "with torch.no_grad():\n",
        "  w = net.layer_inc.double_conv[0].weight.cpu().numpy()\n",
        "nr = int(np.ceil(np.sqrt(w.shape[0])))\n",
        "nc = int(np.ceil(w.shape[0] / nr))\n",
        "fig, ax = plt.subplots(nr, nc)\n",
        "for i in range(w.shape[0]):\n",
        "  r, c = np.unravel_index(i, (nr, nc))\n",
        "  fil = np.transpose(w[i, :, :, :], [1, 2, 0])\n",
        "  if fil.shape[-1] == 1:\n",
        "    fil = fil[:, :, 0]\n",
        "  ax[r][c].imshow(fil)\n",
        "  plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "s-q6eqjFMbor"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Final evaluation on the test set\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "fpfyNp07Mbor",
        "cellView": "form"
      },
      "source": [
        "#@title load test data\n",
        "# final evaluation on the test set. for proper evaluation, and to avoid overfitting\n",
        "# to the test set, we want to change parameters based on the validation set, and\n",
        "# only at the very end evaluate on the test set\n",
        "\n",
        "with open(testannfile) as f:\n",
        "  testann = json.load(f)\n",
        "f.close()\n",
        "ntestims = len(testann['images'])\n",
        "# Make sure we have all the images\n",
        "t = glob(os.path.join(testdir, '*.png'))\n",
        "print(f\"N. test images = {ntestims}, number of images unzipped = {len(t)}\")\n",
        "assert ntestims==len(t), 'number of annotations and number of images do not match'\n",
        "\n",
        "test_dataset = COCODataset(testannfile, datadir=testdir, landmarks=landmarks)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                              batch_size=2,\n",
        "                                              shuffle=True)\n",
        "\n",
        "l2err_per_landmark_test, test_ids = eval_net(net, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "bYE8rP3vMbos",
        "cellView": "form"
      },
      "source": [
        "#@title Plot the error distribution\n",
        "nbins = 25\n",
        "bin_edges = np.linspace(0, np.percentile(l2err_per_landmark_val, 99.),\n",
        "                        nbins + 1)\n",
        "bin_centers = (bin_edges[1:] + bin_edges[:-1]) / 2.\n",
        "bin_edges[-1] = np.inf\n",
        "frac_val = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
        "frac_train = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
        "frac_test = np.zeros((nbins, l2err_per_landmark_val.shape[1]))\n",
        "for i in range(l2err_per_landmark_val.shape[1]):\n",
        "  frac_val[:, i], _ = np.histogram(l2err_per_landmark_val[:, i],\n",
        "                                   bin_edges, density=True)\n",
        "  frac_train[:, i], _ = np.histogram(l2err_per_landmark_train[:, i],\n",
        "                                     bin_edges, density=True)\n",
        "  frac_test[:, i], _ = np.histogram(l2err_per_landmark_test[:, i],\n",
        "                                    bin_edges, density=True)\n",
        "\n",
        "fig=plt.figure(figsize=(8, 4 * train_dataset.nlandmarks), dpi=100)\n",
        "for i in range(train_dataset.nlandmarks):\n",
        "  if landmarks is None:\n",
        "    landmark_name = landmark_names[i]\n",
        "  else:\n",
        "    landmark_name = landmark_names[landmarks[i]]\n",
        "  plt.subplot(train_dataset.nlandmarks, 1, i + 1)\n",
        "  plt.plot(bin_centers, frac_test[:, i], '.-',\n",
        "           label='Test', color=colors[i, :])\n",
        "  plt.plot(bin_centers, frac_val[:, i], '--',\n",
        "           label='Val', color=colors[i, :])\n",
        "  plt.plot(bin_centers, frac_train[:, i], ':',\n",
        "           label='Train', color=colors[i, :])\n",
        "  plt.legend()\n",
        "  plt.title(f\"{landmark_name} error (px)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "BrWLKCiHMbos",
        "cellView": "form"
      },
      "source": [
        "#@title Plot examples with big errors\n",
        "idx = np.argsort(-np.sum(l2err_per_landmark_test, axis=1))\n",
        "\n",
        "i = 19\n",
        "while -np.sum(l2err_per_landmark_test[idx[i]]) < -100 and i < 40:\n",
        "  d = test_dataset[test_ids[idx[i]]]\n",
        "  img = d['image'].unsqueeze(0)\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    pred = net.output(img.to(device))\n",
        "\n",
        "  fig=plt.figure(figsize=(12, 4), dpi=100)\n",
        "  with np.printoptions(precision=2):\n",
        "    errstr = str(l2err_per_landmark_test[idx[i]])\n",
        "  PlotLabelAndPrediction(d, pred[0, ...])  #,title_string='Err = %s '%errstr)\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}